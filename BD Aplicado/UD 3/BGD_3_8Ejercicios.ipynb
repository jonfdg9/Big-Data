{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CkMrsp85SQ4"
      },
      "source": [
        "# EJERCICIOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LuecDqKh5SRT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, current_date, year, monotonically_increasing_id,  avg, min, coalesce, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"pyspark_rdd\").getOrCreate()\n",
        "#spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxPbn7ct5SRW"
      },
      "source": [
        "## EJERCICIO 0\n",
        "En un documento word haz una lista de las diferentes operaciones con una breve descripción de lo que hace y un ejemplo de como se utiliza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Operaciones comunes con DataFrames en Spark\n",
        "\n",
        "## 1. **Creación de un DataFrame**\n",
        "   - **Descripción**: Crear un DataFrame a partir de una lista, un RDD, o una fuente de datos externa como un archivo CSV, JSON, etc.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     from pyspark.sql import SparkSession\n",
        "\n",
        "     spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "     data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
        "     columns = [\"Name\", \"Age\"]\n",
        "     df = spark.createDataFrame(data, columns)\n",
        "     df.show()\n",
        "     ```\n",
        "\n",
        "## 2. **Selección de columnas**\n",
        "   - **Descripción**: Seleccionar una o más columnas de un DataFrame.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df.select(\"Name\").show()\n",
        "     ```\n",
        "\n",
        "## 3. **Filtrado de filas**\n",
        "   - **Descripción**: Filtrar filas basadas en una condición.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df.filter(df[\"Age\"] > 30).show()\n",
        "     ```\n",
        "\n",
        "## 4. **Agregación**\n",
        "   - **Descripción**: Realizar operaciones de agregación como `count`, `sum`, `avg`, `min`, `max`, etc.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     from pyspark.sql import functions as F\n",
        "\n",
        "     df.agg(F.max(\"Age\")).show()\n",
        "     ```\n",
        "\n",
        "## 5. **Ordenación**\n",
        "   - **Descripción**: Ordenar el DataFrame por una o más columnas.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df.orderBy(\"Age\", ascending=False).show()\n",
        "     ```\n",
        "\n",
        "## 6. **Agrupación**\n",
        "   - **Descripción**: Agrupar datos basados en una o más columnas y luego aplicar una función de agregación.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df.groupBy(\"Name\").agg(F.sum(\"Age\")).show()\n",
        "     ```\n",
        "\n",
        "## 7. **Unión de DataFrames**\n",
        "   - **Descripción**: Unir dos DataFrames verticalmente (añadir filas).\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df2 = spark.createDataFrame([(\"David\", 50)], columns)\n",
        "     df.union(df2).show()\n",
        "     ```\n",
        "\n",
        "## 8. **Join de DataFrames**\n",
        "   - **Descripción**: Unir dos DataFrames horizontalmente basado en una clave común.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df3 = spark.createDataFrame([(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\")], [\"Name\", \"Profession\"])\n",
        "     df.join(df3, on=\"Name\", how=\"inner\").show()\n",
        "     ```\n",
        "\n",
        "## 9. **Renombrar columnas**\n",
        "   - **Descripción**: Cambiar el nombre de una o más columnas.\n",
        "   - **Ejemplo**:\n",
        "     ```python\n",
        "     df.withColumnRenamed(\"Age\", \"Years\").show()\n",
        "     ```\n",
        "\n",
        "## 10. **Añadir una nueva columna**\n",
        "  - **Descripción**: Añadir una nueva columna basada en una expresión o transformación.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df = df.withColumn(\"AgePlus10\", df[\"Age\"] + 10)\n",
        "      df.show()\n",
        "      ```\n",
        "\n",
        "## 11. **Eliminar columnas**\n",
        "  - **Descripción**: Eliminar una o más columnas del DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df = df.drop(\"AgePlus10\")\n",
        "      df.show()\n",
        "      ```\n",
        "\n",
        "## 12. **Distinct**\n",
        "  - **Descripción**: Obtener filas únicas basadas en todas las columnas o un subconjunto de columnas.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.select(\"Name\").distinct().show()\n",
        "      ```\n",
        "\n",
        "## 13. **Persistencia (Caching)**\n",
        "  - **Descripción**: Almacenar en caché un DataFrame para mejorar el rendimiento en operaciones repetitivas.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.cache()\n",
        "      df.count()  # Para materializar el caché\n",
        "      ```\n",
        "\n",
        "## 14. **Escritura en disco**\n",
        "  - **Descripción**: Guardar un DataFrame en un archivo (CSV, JSON, Parquet, etc.).\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.write.csv(\"output.csv\", header=True)\n",
        "      ```\n",
        "\n",
        "## 15. **Lectura desde disco**\n",
        "  - **Descripción**: Leer un DataFrame desde un archivo (CSV, JSON, Parquet, etc.).\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df = spark.read.csv(\"output.csv\", header=True, inferSchema=True)\n",
        "      df.show()\n",
        "      ```\n",
        "\n",
        "## 16. **Explode**\n",
        "  - **Descripción**: Convertir una columna de tipo array o map en múltiples filas.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      from pyspark.sql.functions import explode\n",
        "\n",
        "      data = [(\"Alice\", [1, 2, 3]), (\"Bob\", [4, 5])]\n",
        "      columns = [\"Name\", \"Numbers\"]\n",
        "      df = spark.createDataFrame(data, columns)\n",
        "      df.withColumn(\"Number\", explode(\"Numbers\")).show()\n",
        "      ```\n",
        "\n",
        "## 17. **UDF (User Defined Functions)**\n",
        "  - **Descripción**: Definir y utilizar una función personalizada en Spark.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      from pyspark.sql.functions import udf\n",
        "      from pyspark.sql.types import IntegerType\n",
        "\n",
        "      def square(x):\n",
        "          return x * x\n",
        "\n",
        "      square_udf = udf(square, IntegerType())\n",
        "      df.withColumn(\"AgeSquared\", square_udf(df[\"Age\"])).show()\n",
        "      ```\n",
        "\n",
        "## 18. **Window Functions**\n",
        "  - **Descripción**: Realizar operaciones sobre una ventana de filas (por ejemplo, ranking, sumas acumulativas).\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      from pyspark.sql.window import Window\n",
        "      from pyspark.sql.functions import row_number\n",
        "\n",
        "      windowSpec = Window.orderBy(\"Age\")\n",
        "      df.withColumn(\"row_number\", row_number().over(windowSpec)).show()\n",
        "      ```\n",
        "\n",
        "## 19. **Pivot**\n",
        "  - **Descripción**: Convertir valores de una columna en múltiples columnas (pivote).\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      data = [(\"Alice\", \"Math\", 80), (\"Alice\", \"Science\", 90), (\"Bob\", \"Math\", 85)]\n",
        "      columns = [\"Name\", \"Subject\", \"Score\"]\n",
        "      df = spark.createDataFrame(data, columns)\n",
        "      df.groupBy(\"Name\").pivot(\"Subject\").avg(\"Score\").show()\n",
        "      ```\n",
        "\n",
        "## 20. **Describe**\n",
        "  - **Descripción**: Obtener estadísticas descriptivas de las columnas numéricas.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.describe().show()\n",
        "      ```\n",
        "\n",
        "## 21. **Drop Duplicates**\n",
        "  - **Descripción**: Eliminar filas duplicadas basadas en todas las columnas o un subconjunto de columnas.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.dropDuplicates([\"Name\"]).show()\n",
        "      ```\n",
        "\n",
        "## 22. **Alias**\n",
        "  - **Descripción**: Asignar un alias a una columna o tabla.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.select(df[\"Name\"].alias(\"Nombre\")).show()\n",
        "      ```\n",
        "\n",
        "## 23. **Collect**\n",
        "  - **Descripción**: Recopilar todas las filas del DataFrame como una lista en el driver.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      rows = df.collect()\n",
        "      for row in rows:\n",
        "          print(row)\n",
        "      ```\n",
        "\n",
        "## 24. **Take**\n",
        "  - **Descripción**: Obtener un número específico de filas del DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      rows = df.take(2)\n",
        "      for row in rows:\n",
        "          print(row)\n",
        "      ```\n",
        "\n",
        "## 25. **Schema**\n",
        "  - **Descripción**: Obtener el esquema del DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.printSchema()\n",
        "      ```\n",
        "\n",
        "## 26. **WithColumn**\n",
        "  - **Descripción**: Añadir o reemplazar una columna en el DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df = df.withColumn(\"AgePlus10\", df[\"Age\"] + 10)\n",
        "      df.show()\n",
        "      ```\n",
        "\n",
        "## 27. **Cast**\n",
        "  - **Descripción**: Cambiar el tipo de datos de una columna.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      from pyspark.sql.functions import col\n",
        "\n",
        "      df = df.withColumn(\"Age\", col(\"Age\").cast(\"String\"))\n",
        "      df.printSchema()\n",
        "      ```\n",
        "\n",
        "## 28. **Sample**\n",
        "   - **Descripción**: Obtener una muestra aleatoria de filas del DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df.sample(0.5).show()\n",
        "      ```\n",
        "\n",
        "## 29. **Repartition**\n",
        "   - **Descripción**: Cambiar el número de particiones del DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df = df.repartition(4)\n",
        "      print(df.rdd.getNumPartitions())\n",
        "      ```\n",
        "\n",
        "## 30. **Coalesce**\n",
        "  - **Descripción**: Reducir el número de particiones del DataFrame.\n",
        "    - **Ejemplo**:\n",
        "      ```python\n",
        "      df = df.coalesce(2)\n",
        "      print(df.rdd.getNumPartitions())\n",
        "      ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ywlD0T5SRX"
      },
      "source": [
        "## EJERCICIO 1\n",
        "Realiza las siguientes operaciones:\n",
        "* Importa el csv de \"data/WorldCupPlayers.csv\" (que deduzca el esquema)\n",
        "* Visualiza los datos\n",
        "* ¿Que tipo de datos contiene cada variable?\n",
        "* ¿Cuantos registros hay?\n",
        "* Obtén los principales estadísticos de Position\n",
        "* Selecciona y muestra los \"Team initials\" diferentes que hay ¿Cuántos hay?\n",
        "* ¿Cuantos partidos con el ID de 1096 ha habido?\n",
        "* Muestra los datos donde la posicion haya sido C y el evento sea G40\n",
        "* Utiliza Spark SQL para mostras los registros donde el MatchID sea mayor o igual a 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QnmFQq55SRX",
        "outputId": "cbfb95e7-dd96-40bf-c9fe-80a07a6bad62"
      },
      "outputs": [],
      "source": [
        "# Se lee el archivo CSV con encabezado y se infiere el esquema automáticamente\n",
        "df = spark.read.csv(\"./WorldCupPlayers.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualización de los datos:\n",
            "+-------+-------+-------------+-------------------+-------+------------+----------------+--------+-----+\n",
            "|RoundID|MatchID|Team Initials|         Coach Name|Line-up|Shirt Number|     Player Name|Position|Event|\n",
            "+-------+-------+-------------+-------------------+-------+------------+----------------+--------+-----+\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|     Alex THEPOT|      GK| NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0| Oscar BONFIGLIO|      GK| NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|Marcel LANGILLER|    NULL| G40'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|    Juan CARRENO|    NULL| G70'|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0| Ernest LIBERATI|    NULL| NULL|\n",
            "+-------+-------+-------------+-------------------+-------+------------+----------------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mostrar las primeras 5 filas del DataFrame para inspeccionar los datos\n",
        "print(\"Visualización de los datos:\")\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- RoundID: integer (nullable = true)\n",
            " |-- MatchID: integer (nullable = true)\n",
            " |-- Team Initials: string (nullable = true)\n",
            " |-- Coach Name: string (nullable = true)\n",
            " |-- Line-up: string (nullable = true)\n",
            " |-- Shirt Number: integer (nullable = true)\n",
            " |-- Player Name: string (nullable = true)\n",
            " |-- Position: string (nullable = true)\n",
            " |-- Event: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mostrar el esquema del DataFrame para conocer los tipos de datos de cada columna\n",
        "print(\"\\nEsquema del DataFrame:\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Número de registros: 37784\n"
          ]
        }
      ],
      "source": [
        "# Contar cuántos registros hay en el DataFrame\n",
        "num_records = df.count()\n",
        "print(f\"\\nNúmero de registros: {num_records}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estadísticos de la columna 'Position':\n",
            "+-------+--------+\n",
            "|summary|Position|\n",
            "+-------+--------+\n",
            "|  count|    4143|\n",
            "|   mean|    NULL|\n",
            "| stddev|    NULL|\n",
            "|    min|       C|\n",
            "|    max|     GKC|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Usar la función describe() para obtener estadísticas como count, mean, stddev, min y max de position\n",
        "print(\"\\nEstadísticos de la columna 'Position':\")\n",
        "df.describe(\"Position\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|Team Initials|\n",
            "+-------------+\n",
            "|          POL|\n",
            "|          JAM|\n",
            "|          BRA|\n",
            "|          CUB|\n",
            "|          FRA|\n",
            "|          ALG|\n",
            "|          BOL|\n",
            "|          RSA|\n",
            "|          ITA|\n",
            "|          UKR|\n",
            "|          CMR|\n",
            "|          SCG|\n",
            "|          GHA|\n",
            "|          SEN|\n",
            "|          TOG|\n",
            "|          TRI|\n",
            "|          TCH|\n",
            "|          AUS|\n",
            "|          MEX|\n",
            "|          PAR|\n",
            "+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Número de 'Team Initials' diferentes: 82\n"
          ]
        }
      ],
      "source": [
        "# Seleccionar los valores únicos de la columna \"Team Initials\"\n",
        "team_initials = df.select(\"Team Initials\").distinct()\n",
        "team_initials.show()\n",
        "# Contar cuántos valores únicos hay en \"Team Initials\"\n",
        "num_team_initials = team_initials.count()\n",
        "print(f\"\\nNúmero de 'Team Initials' diferentes: {num_team_initials}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Número de partidos con el ID 1096: 33\n"
          ]
        }
      ],
      "source": [
        "# Filtrar el DataFrame para contar los registros donde MatchID sea 1096\n",
        "num_matches_1096 = df.filter(df.MatchID == 1096).count()\n",
        "print(f\"\\nNúmero de partidos con el ID 1096: {num_matches_1096}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+-------------+--------------------+-------+------------+----------------+--------+-----+\n",
            "|RoundID|MatchID|Team Initials|          Coach Name|Line-up|Shirt Number|     Player Name|Position|Event|\n",
            "+-------+-------+-------------+--------------------+-------+------------+----------------+--------+-----+\n",
            "|    201|   1089|          PAR|DURAND LAGUNA Jos...|      S|           0|Luis VARGAS PENA|       C| G40'|\n",
            "|    429|   1175|          HUN|  DIETZ Karoly (HUN)|      S|           0|   Gyorgy SAROSI|       C| G40'|\n",
            "+-------+-------+-------------+--------------------+-------+------------+----------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filtrar el DataFrame para obtener registros donde Position sea \"C\" y Event sea \"G40\"\n",
        "filtered_data = df.filter((df.Position == \"C\") & (df.Event == \"G40'\"))\n",
        "filtered_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "|RoundID|MatchID|Team Initials|         Coach Name|Line-up|Shirt Number|      Player Name|Position|    Event|\n",
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|      Alex THEPOT|      GK|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|  Oscar BONFIGLIO|      GK|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0| Marcel LANGILLER|    NULL|     G40'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Juan CARRENO|    NULL|     G70'|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Ernest LIBERATI|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Rafael GARZA|       C|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Andre MASCHINOT|    NULL|G43' G87'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|    Hilario LOPEZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Etienne MATTLER|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|   Dionisio MEJIA|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|     Marcel PINEL|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Felipe ROSAS|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|  Alex VILLAPLANE|       C|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|     Manuel ROSAS|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|   Lucien LAURENT|    NULL|     G19'|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|        Jose RUIZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|   Marcel CAPELLE|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|  Alfredo SANCHEZ|    NULL|     NULL|\n",
            "|    201|   1096|          FRA|CAUDRON Raoul (FRA)|      S|           0|Augustin CHANTREL|    NULL|     NULL|\n",
            "|    201|   1096|          MEX|   LUQUE Juan (MEX)|      S|           0|   Efrain AMEZCUA|    NULL|     NULL|\n",
            "+-------+-------+-------------+-------------------+-------+------------+-----------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Registrar el DataFrame como una vista temporal para poder usar Spark SQL\n",
        "df.createOrReplaceTempView(\"world_cup_players\")\n",
        "\n",
        "# Ejecutar una consulta SQL para filtrar registros donde MatchID sea mayor o igual a 20\n",
        "result = spark.sql(\"SELECT * FROM world_cup_players WHERE MatchID >= 20\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwlXPat85SRc"
      },
      "source": [
        "## EJERCICIO 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySES8yYI5SRc"
      },
      "source": [
        "A partir del archivo nombres.json, crea un DataFrame y realiza las siguientes operaciones:\n",
        "\n",
        "1. Crea una nueva columna (columna Mayor30) que indique si la persona es mayor de 30 años.\n",
        "2. Crea una nueva columna (columna FaltanJubilacion) que calcule cuantos años le faltan para jubilarse (supongamos que se jubila a los 67 años)\n",
        "3. Crea una nueva columna (columna Apellidos) que contenga XYZ (puedes utilizar la función lit)\n",
        "4. Elimina las columna Mayor30 y Apellidos.\n",
        "5. Crea una nueva columna (columna AnyoNac) con el año de nacimiento de cada persona (puedes utilizar la función current_date).\n",
        "6. Añade un id incremental para cada fila (campo Id) y haz que al hacer un show se vea en primer lugar (puedes utilizar la función monotonically_increasing_id) seguidos del Nombre, Edad, AnyoNac, FaltaJubilacion y Ciudad\n",
        "\n",
        "Al realizar los seis pasos, el resultado del DataFrame será similar a :\n",
        "``````\n",
        "+---+-------+----+-------+----------------+--------+\n",
        "| Id|Nombre |Edad|AnyoNac|FaltanJubilacion|  Ciudad|\n",
        "+---+-------+----+-------+----------------+--------+\n",
        "|  0|  Aitor|  45|   1977|              22|   Elche|\n",
        "|  1| Marina|  14|   2008|              53|Alicante|\n",
        "|  2|  Laura|  19|   2003|              48|   Elche|\n",
        "|  3|  Sonia|  45|   1977|              22|    Aspe|\n",
        "|  4|  Pedro|null|   null|            null|   Elche|\n",
        "+---+-------+----+-------+----------------+--------+\n",
        "``````"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpLo_k6g5SRc",
        "outputId": "4a0cebf1-589b-4053-ad62-db2740038016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+---+-------+----------------+---------+\n",
            "|  Ciudad|Edad|Nombre| Id|Mayor30|FaltanJubilacion|Apellidos|\n",
            "+--------+----+------+---+-------+----------------+---------+\n",
            "|   Elche|  45| Aitor|  0|   true|              22|      XYZ|\n",
            "|Alicante|  14|Marina|  1|  false|              53|      XYZ|\n",
            "|   Elche|  19| Laura|  2|  false|              48|      XYZ|\n",
            "|    Aspe|  45| Sonia|  3|   true|              22|      XYZ|\n",
            "|   Elche|NULL| Pedro|  4|   NULL|            NULL|      XYZ|\n",
            "+--------+----+------+---+-------+----------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = spark.read.json(\"./nombres.json\")\n",
        "df2= df2.withColumn(\"Id\", monotonically_increasing_id())\n",
        "df2=df2.withColumn(\"Mayor30\", df2[\"edad\"]>30)\n",
        "df2=df2.withColumn(\"FaltanJubilacion\", 67-df2[\"edad\"])\n",
        "df2=df2.withColumn(\"Apellidos\", lit(\"XYZ\")) \n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----+------+---+----------------+\n",
            "|  Ciudad|Edad|Nombre| Id|FaltanJubilacion|\n",
            "+--------+----+------+---+----------------+\n",
            "|   Elche|  45| Aitor|  0|              22|\n",
            "|Alicante|  14|Marina|  1|              53|\n",
            "|   Elche|  19| Laura|  2|              48|\n",
            "|    Aspe|  45| Sonia|  3|              22|\n",
            "|   Elche|NULL| Pedro|  4|            NULL|\n",
            "+--------+----+------+---+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2=df2.drop(\"Apellidos\", \"Mayor30\")\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----+-------+----------------+--------+\n",
            "| Id|Nombre|Edad|AnyoNac|FaltanJubilacion|  Ciudad|\n",
            "+---+------+----+-------+----------------+--------+\n",
            "|  0| Aitor|  45|   1980|              22|   Elche|\n",
            "|  1|Marina|  14|   2011|              53|Alicante|\n",
            "|  2| Laura|  19|   2006|              48|   Elche|\n",
            "|  3| Sonia|  45|   1980|              22|    Aspe|\n",
            "|  4| Pedro|NULL|   NULL|            NULL|   Elche|\n",
            "+---+------+----+-------+----------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2=df2.withColumn(\"AnyoNac\", year(current_date()) - col(\"edad\"))\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----+-------+----------------+--------+\n",
            "| Id|Nombre|Edad|AnyoNac|FaltanJubilacion|  Ciudad|\n",
            "+---+------+----+-------+----------------+--------+\n",
            "|  0| Aitor|  45|   1980|              22|   Elche|\n",
            "|  1|Marina|  14|   2011|              53|Alicante|\n",
            "|  2| Laura|  19|   2006|              48|   Elche|\n",
            "|  3| Sonia|  45|   1980|              22|    Aspe|\n",
            "|  4| Pedro|NULL|   NULL|            NULL|   Elche|\n",
            "+---+------+----+-------+----------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = df2.select(\"Id\", \"Nombre\", \"Edad\", \"AnyoNac\", \"FaltanJubilacion\", \"Ciudad\")\n",
        "\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZhUS4RU5SRd"
      },
      "source": [
        "## EJERCICIO 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0VRuSPS5SRd"
      },
      "source": [
        "A partir del archivo VentasNulos.csv:\n",
        "\n",
        "1. Elimina las filas que tengan al menos 4 nulos.\n",
        "\n",
        "2. Con las filas restantes, sustituye:\n",
        "\n",
        "    * Los nombres nulos por Empleado\n",
        "    * Las ventas nulas por la media de las ventas de los compañeros (redondeado a entero).\n",
        "    ``````\n",
        "        media = df.groupBy().avg('Ventas')\n",
        "    ``````\n",
        "    * Los euros nulos por el valor del compañero que menos € ha ganado. (tras agrupar, puedes usar la función min)\n",
        "    * La ciudad nula por C.V. y el identificador nulo por XYZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma0jMrYh5SRe",
        "outputId": "efe5c87e-8e98-45c3-96e3-d44befd97f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+-----------+-------------+\n",
            "|Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+------+------+-----+-----------+-------------+\n",
            "|  Pepe|     4|  200|      Elche|          X21|\n",
            "|Andreu|     8| NULL|       NULL|         NULL|\n",
            "|  Juan|  NULL| NULL|       NULL|          C54|\n",
            "| Pedro|     1|   30|   Valencia|          R23|\n",
            "| María|  NULL|  300| Torrellano|         NULL|\n",
            "|Marina|     3|  350|       Aspe|          V55|\n",
            "|  NULL|    10|  500|Crevillente|          AMV|\n",
            "|   Ana|    10| 2300|   Alicante|          B89|\n",
            "|  NULL|  NULL| NULL|       NULL|         NULL|\n",
            "| Jorge|     8| NULL|       NULL|          T19|\n",
            "+------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3 = spark.read.csv(\"./VentasNulos.csv\", header=True, inferSchema=True)\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+-----+-----------+-------------+\n",
            "|Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+------+------+-----+-----------+-------------+\n",
            "|  Pepe|     4|  200|      Elche|          X21|\n",
            "|Andreu|     8| NULL|       NULL|         NULL|\n",
            "|  Juan|  NULL| NULL|       NULL|          C54|\n",
            "| Pedro|     1|   30|   Valencia|          R23|\n",
            "| María|  NULL|  300| Torrellano|         NULL|\n",
            "|Marina|     3|  350|       Aspe|          V55|\n",
            "|  NULL|    10|  500|Crevillente|          AMV|\n",
            "|   Ana|    10| 2300|   Alicante|          B89|\n",
            "| Jorge|     8| NULL|       NULL|          T19|\n",
            "+------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3= df3.dropna(thresh=1)\n",
        "df3.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+-----+-----------+-------------+\n",
            "|  Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "|    Pepe|     4|  200|      Elche|          X21|\n",
            "|  Andreu|     8| NULL|       NULL|         NULL|\n",
            "|    Juan|  NULL| NULL|       NULL|          C54|\n",
            "|   Pedro|     1|   30|   Valencia|          R23|\n",
            "|   María|  NULL|  300| Torrellano|         NULL|\n",
            "|  Marina|     3|  350|       Aspe|          V55|\n",
            "|Empleado|    10|  500|Crevillente|          AMV|\n",
            "|     Ana|    10| 2300|   Alicante|          B89|\n",
            "|   Jorge|     8| NULL|       NULL|          T19|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3=df3.fillna(\"Empleado\", subset=[\"Nombre\"])\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+-----+-----------+-------------+\n",
            "|  Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "|    Pepe|     4|  200|      Elche|          X21|\n",
            "|  Andreu|     8| NULL|       NULL|         NULL|\n",
            "|    Juan|     6| NULL|       NULL|          C54|\n",
            "|   Pedro|     1|   30|   Valencia|          R23|\n",
            "|   María|     6|  300| Torrellano|         NULL|\n",
            "|  Marina|     3|  350|       Aspe|          V55|\n",
            "|Empleado|    10|  500|Crevillente|          AMV|\n",
            "|     Ana|    10| 2300|   Alicante|          B89|\n",
            "|   Jorge|     8| NULL|       NULL|          T19|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "media = df3.groupBy().avg('Ventas')\n",
        "df3 = df3.fillna(media.collect()[0][0], subset=['Ventas'])\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+-----+-----------+-------------+\n",
            "|  Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "|    Pepe|     4|  200|      Elche|          X21|\n",
            "|  Andreu|     8|   30|       NULL|         NULL|\n",
            "|    Juan|     6|   30|       NULL|          C54|\n",
            "|   Pedro|     1|   30|   Valencia|          R23|\n",
            "|   María|     6|  300| Torrellano|         NULL|\n",
            "|  Marina|     3|  350|       Aspe|          V55|\n",
            "|Empleado|    10|  500|Crevillente|          AMV|\n",
            "|     Ana|    10| 2300|   Alicante|          B89|\n",
            "|   Jorge|     8|   30|       NULL|          T19|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "minEuros = df3.groupBy().min('Euros')\n",
        "df3 = df3.fillna(minEuros.collect()[0][0], subset=['Euros'])\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+-----+-----------+-------------+\n",
            "|  Nombre|Ventas|Euros|     Ciudad|Identificador|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "|    Pepe|     4|  200|      Elche|          X21|\n",
            "|  Andreu|     8|   30|       C.V.|          XYZ|\n",
            "|    Juan|     6|   30|       C.V.|          C54|\n",
            "|   Pedro|     1|   30|   Valencia|          R23|\n",
            "|   María|     6|  300| Torrellano|          XYZ|\n",
            "|  Marina|     3|  350|       Aspe|          V55|\n",
            "|Empleado|    10|  500|Crevillente|          AMV|\n",
            "|     Ana|    10| 2300|   Alicante|          B89|\n",
            "|   Jorge|     8|   30|       C.V.|          T19|\n",
            "+--------+------+-----+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df3 = df3.fillna(\"C.V.\", subset=[\"Ciudad\"])\n",
        "df3=df3.fillna(\"XYZ\", subset=[\"Identificador\"])\n",
        "df3.show()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62C3pGjg5SRe"
      },
      "source": [
        "## EJERCICIO 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz8oZ4zB5SRe"
      },
      "source": [
        " A partir del archivo movies.tsv, crea una esquema de forma declarativa con los campos:\n",
        "\n",
        "* interprete de tipo string\n",
        "* pelicula de tipo string\n",
        "* anyo de tipo int\n",
        "\n",
        "Cada fila del fichero implica que el actor/actriz ha trabajado en dicha película en el año indicado.\n",
        "1. Una vez creado el esquema, carga los datos en un DataFrame.\n",
        "\n",
        "A continuación, mediante el DataFrame API:\n",
        "\n",
        "2. Muestra las películas en las que ha trabajado Murphy, Eddie (I).\n",
        "3. Muestra los intérpretes que aparecen tanto en Superman como en Superman II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "SeFxQSdgbuti"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+--------------------+----+\n",
            "|       interprete|            pelicula|anyo|\n",
            "+-----------------+--------------------+----+\n",
            "|McClure, Marc (I)|        Coach Carter|2005|\n",
            "|McClure, Marc (I)|         Superman II|1980|\n",
            "|McClure, Marc (I)|           Apollo 13|1995|\n",
            "|McClure, Marc (I)|            Superman|1978|\n",
            "|McClure, Marc (I)|  Back to the Future|1985|\n",
            "|McClure, Marc (I)|Back to the Futur...|1990|\n",
            "|Cooper, Chris (I)|  Me, Myself & Irene|2000|\n",
            "|Cooper, Chris (I)|         October Sky|1999|\n",
            "|Cooper, Chris (I)|              Capote|2005|\n",
            "|Cooper, Chris (I)|The Bourne Supremacy|2004|\n",
            "|Cooper, Chris (I)|         The Patriot|2000|\n",
            "|Cooper, Chris (I)|            The Town|2010|\n",
            "|Cooper, Chris (I)|          Seabiscuit|2003|\n",
            "|Cooper, Chris (I)|      A Time to Kill|1996|\n",
            "|Cooper, Chris (I)|Where the Wild Th...|2009|\n",
            "|Cooper, Chris (I)|         The Muppets|2011|\n",
            "|Cooper, Chris (I)|     American Beauty|1999|\n",
            "|Cooper, Chris (I)|             Syriana|2005|\n",
            "|Cooper, Chris (I)| The Horse Whisperer|1998|\n",
            "|Cooper, Chris (I)|             Jarhead|2005|\n",
            "+-----------------+--------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:18:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: McClure, Marc (I), Freaky Friday, 2003\n",
            " Schema: interprete, pelicula, anyo\n",
            "Expected: interprete but found: McClure, Marc (I)\n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/movies.tsv\n"
          ]
        }
      ],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"interprete\", StringType(), True),\n",
        "    StructField(\"pelicula\", StringType(), True),\n",
        "    StructField(\"anyo\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df4 = spark.read.csv(\"./movies.tsv\", sep=\"\\t\", header=True, schema=schema)\n",
        "df4.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+--------------------+----+\n",
            "|       interprete|            pelicula|anyo|\n",
            "+-----------------+--------------------+----+\n",
            "|Murphy, Eddie (I)|            Showtime|2002|\n",
            "|Murphy, Eddie (I)|              Norbit|2007|\n",
            "|Murphy, Eddie (I)|Hot Tub Time Machine|2010|\n",
            "|Murphy, Eddie (I)|Nutty Professor I...|2000|\n",
            "|Murphy, Eddie (I)|Beverly Hills Cop II|1987|\n",
            "|Murphy, Eddie (I)|      Trading Places|1983|\n",
            "|Murphy, Eddie (I)|      Daddy Day Care|2003|\n",
            "|Murphy, Eddie (I)|      Dr. Dolittle 2|2001|\n",
            "|Murphy, Eddie (I)| Shrek Forever After|2010|\n",
            "|Murphy, Eddie (I)|   Beverly Hills Cop|1984|\n",
            "|Murphy, Eddie (I)|               Shrek|2001|\n",
            "|Murphy, Eddie (I)| The Haunted Mansion|2003|\n",
            "|Murphy, Eddie (I)|   Coming to America|1988|\n",
            "|Murphy, Eddie (I)|             Shrek 2|2004|\n",
            "|Murphy, Eddie (I)|     Doctor Dolittle|1998|\n",
            "|Murphy, Eddie (I)| The Nutty Professor|1996|\n",
            "|Murphy, Eddie (I)|               Mulan|1998|\n",
            "|Murphy, Eddie (I)|         Tower Heist|2011|\n",
            "|Murphy, Eddie (I)|          Dreamgirls|2006|\n",
            "|Murphy, Eddie (I)|           Bowfinger|1999|\n",
            "+-----------------+--------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:19:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: McClure, Marc (I), Freaky Friday, 2003\n",
            " Schema: interprete, pelicula, anyo\n",
            "Expected: interprete but found: McClure, Marc (I)\n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/movies.tsv\n"
          ]
        }
      ],
      "source": [
        "murphy = df4.filter(df4.interprete == \"Murphy, Eddie (I)\")\n",
        "murphy.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+----+\n",
            "|          interprete|   pelicula|anyo|\n",
            "+--------------------+-----------+----+\n",
            "|   McClure, Marc (I)|Superman II|1980|\n",
            "|   McClure, Marc (I)|   Superman|1978|\n",
            "|    Lipinski, Eugene|Superman II|1980|\n",
            "|       Calder, David|   Superman|1978|\n",
            "|      Brando, Marlon|   Superman|1978|\n",
            "|        Tuerpe, Paul|   Superman|1978|\n",
            "|   Marzello, Vincent|   Superman|1978|\n",
            "|Griffiths, Richar...|Superman II|1980|\n",
            "|    Perrine, Valerie|   Superman|1978|\n",
            "|    Perrine, Valerie|Superman II|1980|\n",
            "|     Chancer, Norman|Superman II|1980|\n",
            "|       Hackman, Gene|Superman II|1980|\n",
            "|       Hackman, Gene|   Superman|1978|\n",
            "|      Stamp, Terence|   Superman|1978|\n",
            "|      Stamp, Terence|Superman II|1980|\n",
            "|    Hollis, John (I)|   Superman|1978|\n",
            "|    Hollis, John (I)|Superman II|1980|\n",
            "|        Lyons, Derek|Superman II|1980|\n",
            "|        Kahan, Steve|   Superman|1978|\n",
            "|    Jurgensen, Randy|   Superman|1978|\n",
            "+--------------------+-----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:23:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: McClure, Marc (I), Freaky Friday, 2003\n",
            " Schema: interprete, pelicula, anyo\n",
            "Expected: interprete but found: McClure, Marc (I)\n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/movies.tsv\n"
          ]
        }
      ],
      "source": [
        "superman= df4.filter((df4.pelicula == \"Superman\") | (df4.pelicula == \"Superman II\"))\n",
        "superman.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZ9aGeq5SRf"
      },
      "source": [
        "## EJERCICIO 5\n",
        "Realiza las siguientes operaciones:\n",
        "* Carga el dataset de “data/stocks_price_final.csv”, con el esquema correcto de datos (tienes que crear tu el schema\").\n",
        "* Renombra la variable market.cap a market\n",
        "* Elimina la variable market\n",
        "* Muestra las filas donde el valor de \"open\" es nulo.\n",
        "* Elimina las filas donde el valor de \"open\" es nulo.\n",
        "* Para comprobar el punto anterior vuelve a mostrar las filas donde el valor de \"open\" es nulo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oawpRKQ55SRf",
        "outputId": "21b4b449-0fe5-480f-f9bd-7b3e2e1ff6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
            "| id|symbol|      date|     open|     high|      low|    close| volume| adjusted|market.cap|       sector|            industry|exchange|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
            "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  6|   TXG|2019-09-19|62.810001|   63.375|61.029999|61.119999| 425200|61.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  7|   TXG|2019-09-20|61.709999|62.419998|    59.82|     60.5| 392000|     60.5|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  8|   TXG|2019-09-23|60.220001|61.485001|59.939999|60.330002| 137200|60.330002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  9|   TXG|2019-09-24|     61.0|     61.0|     54.0|54.299999| 713800|54.299999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 10|   TXG|2019-09-25|54.459999|55.880001|   52.563|52.759998| 261200|52.759998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 11|   TXG|2019-09-26|52.779999|53.689999|46.619999|49.990002| 596300|49.990002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 12|   TXG|2019-09-27|51.130001|     55.0|50.700001|51.029999| 621300|51.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 13|   TXG|2019-09-30|51.049999|     52.0|    49.25|50.400002| 168900|50.400002|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 14|   TXG|2019-10-01|50.509998|51.919998|     46.0|47.029999| 536300|47.029999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 15|   TXG|2019-10-02|46.779999|    47.23|45.110001|    46.07| 519600|    46.07|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 16|   TXG|2019-10-03|    46.77|48.240002|    45.75|48.119999| 703900|48.119999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 17|   TXG|2019-10-04|     48.0|    53.34|    47.82|51.450001| 322400|51.450001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 18|   TXG|2019-10-07|52.099998|53.220001|49.029999|50.360001| 476600|50.360001|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 19|   TXG|2019-10-08|     50.0|    51.27|     49.0|49.549999| 284100|49.549999|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 20|   TXG|2019-10-09|49.630001|51.525002|49.575001|50.009998| 201100|50.009998|    $9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+----------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:38:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , symbol, date, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
            " Schema: id, symbol, date, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
            "Expected: id but found: \n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/stocks_price_final.csv\n"
          ]
        }
      ],
      "source": [
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),            \n",
        "    StructField(\"symbol\", StringType(), True),          \n",
        "    StructField(\"date\", StringType(), True),            \n",
        "    StructField(\"open\", DoubleType(), True),            \n",
        "    StructField(\"high\", DoubleType(), True),             \n",
        "    StructField(\"low\", DoubleType(), True),              \n",
        "    StructField(\"close\", DoubleType(), True),            \n",
        "    StructField(\"volume\", IntegerType(), True),         \n",
        "    StructField(\"adjusted\", DoubleType(), True),         \n",
        "    StructField(\"market.cap\", StringType(), True),      \n",
        "    StructField(\"sector\", StringType(), True),          \n",
        "    StructField(\"industry\", StringType(), True),        \n",
        "    StructField(\"exchange\", StringType(), True)         \n",
        "])\n",
        "\n",
        "\n",
        "df5 = spark.read.csv(\"./stocks_price_final.csv\", header=True, schema=schema)\n",
        "df5.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----------+---------+---------+---------+---------+-------+---------+------+-------------+--------------------+--------+\n",
            "| id|symbol|      date|     open|     high|      low|    close| volume| adjusted|market|       sector|            industry|exchange|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+------+-------------+--------------------+--------+\n",
            "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  6|   TXG|2019-09-19|62.810001|   63.375|61.029999|61.119999| 425200|61.119999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  7|   TXG|2019-09-20|61.709999|62.419998|    59.82|     60.5| 392000|     60.5|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  8|   TXG|2019-09-23|60.220001|61.485001|59.939999|60.330002| 137200|60.330002|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  9|   TXG|2019-09-24|     61.0|     61.0|     54.0|54.299999| 713800|54.299999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 10|   TXG|2019-09-25|54.459999|55.880001|   52.563|52.759998| 261200|52.759998|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 11|   TXG|2019-09-26|52.779999|53.689999|46.619999|49.990002| 596300|49.990002|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 12|   TXG|2019-09-27|51.130001|     55.0|50.700001|51.029999| 621300|51.029999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 13|   TXG|2019-09-30|51.049999|     52.0|    49.25|50.400002| 168900|50.400002|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 14|   TXG|2019-10-01|50.509998|51.919998|     46.0|47.029999| 536300|47.029999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 15|   TXG|2019-10-02|46.779999|    47.23|45.110001|    46.07| 519600|    46.07|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 16|   TXG|2019-10-03|    46.77|48.240002|    45.75|48.119999| 703900|48.119999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 17|   TXG|2019-10-04|     48.0|    53.34|    47.82|51.450001| 322400|51.450001|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 18|   TXG|2019-10-07|52.099998|53.220001|49.029999|50.360001| 476600|50.360001|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 19|   TXG|2019-10-08|     50.0|    51.27|     49.0|49.549999| 284100|49.549999|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 20|   TXG|2019-10-09|49.630001|51.525002|49.575001|50.009998| 201100|50.009998|$9.31B|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:38:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , symbol, date, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
            " Schema: id, symbol, date, open, high, low, close, volume, adjusted, market.cap, sector, industry, exchange\n",
            "Expected: id but found: \n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/stocks_price_final.csv\n"
          ]
        }
      ],
      "source": [
        "df5=df5.withColumnRenamed(\"market.cap\", \"market\")\n",
        "df5.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----------+---------+---------+---------+---------+-------+---------+-------------+--------------------+--------+\n",
            "| id|symbol|      date|     open|     high|      low|    close| volume| adjusted|       sector|            industry|exchange|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+-------------+--------------------+--------+\n",
            "|  1|   TXG|2019-09-12|     54.0|     58.0|     51.0|    52.75|7326300|    52.75|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  2|   TXG|2019-09-13|    52.75|   54.355|49.150002|    52.27|1025200|    52.27|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  3|   TXG|2019-09-16|52.450001|     56.0|52.009998|55.200001| 269900|55.200001|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  4|   TXG|2019-09-17|56.209999|60.900002|   55.423|56.779999| 602800|56.779999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  5|   TXG|2019-09-18|56.849998|    62.27|55.650002|     62.0|1589600|     62.0|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  6|   TXG|2019-09-19|62.810001|   63.375|61.029999|61.119999| 425200|61.119999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  7|   TXG|2019-09-20|61.709999|62.419998|    59.82|     60.5| 392000|     60.5|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  8|   TXG|2019-09-23|60.220001|61.485001|59.939999|60.330002| 137200|60.330002|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "|  9|   TXG|2019-09-24|     61.0|     61.0|     54.0|54.299999| 713800|54.299999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 10|   TXG|2019-09-25|54.459999|55.880001|   52.563|52.759998| 261200|52.759998|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 11|   TXG|2019-09-26|52.779999|53.689999|46.619999|49.990002| 596300|49.990002|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 12|   TXG|2019-09-27|51.130001|     55.0|50.700001|51.029999| 621300|51.029999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 13|   TXG|2019-09-30|51.049999|     52.0|    49.25|50.400002| 168900|50.400002|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 14|   TXG|2019-10-01|50.509998|51.919998|     46.0|47.029999| 536300|47.029999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 15|   TXG|2019-10-02|46.779999|    47.23|45.110001|    46.07| 519600|    46.07|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 16|   TXG|2019-10-03|    46.77|48.240002|    45.75|48.119999| 703900|48.119999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 17|   TXG|2019-10-04|     48.0|    53.34|    47.82|51.450001| 322400|51.450001|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 18|   TXG|2019-10-07|52.099998|53.220001|49.029999|50.360001| 476600|50.360001|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 19|   TXG|2019-10-08|     50.0|    51.27|     49.0|49.549999| 284100|49.549999|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "| 20|   TXG|2019-10-09|49.630001|51.525002|49.575001|50.009998| 201100|50.009998|Capital Goods|Biotechnology: La...|  NASDAQ|\n",
            "+---+------+----------+---------+---------+---------+---------+-------+---------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:38:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , symbol, date, open, high, low, close, volume, adjusted, sector, industry, exchange\n",
            " Schema: id, symbol, date, open, high, low, close, volume, adjusted, sector, industry, exchange\n",
            "Expected: id but found: \n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/stocks_price_final.csv\n"
          ]
        }
      ],
      "source": [
        "df5=df5.drop(\"market\")\n",
        "df5.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+----------+----+----+----+-----+------+--------+-------------+--------------------+--------+\n",
            "|  id|symbol|      date|open|high| low|close|volume|adjusted|       sector|            industry|exchange|\n",
            "+----+------+----------+----+----+----+-----+------+--------+-------------+--------------------+--------+\n",
            "|4378|  KRKR|2020-05-11|NULL|NULL|NULL| NULL|  NULL|    NULL|Miscellaneous|   Business Services|  NASDAQ|\n",
            "|5747|  NMTR|2020-01-23|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5748|  NMTR|2020-01-24|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5749|  NMTR|2020-01-27|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5750|  NMTR|2020-01-28|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5751|  NMTR|2020-01-29|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5752|  NMTR|2020-01-30|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5753|  NMTR|2020-01-31|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5754|  NMTR|2020-02-03|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5755|  NMTR|2020-02-04|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5756|  NMTR|2020-02-05|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5757|  NMTR|2020-02-06|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5758|  NMTR|2020-02-07|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5759|  NMTR|2020-02-10|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5760|  NMTR|2020-02-11|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5761|  NMTR|2020-02-12|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5762|  NMTR|2020-02-13|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5763|  NMTR|2020-02-14|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5764|  NMTR|2020-02-18|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "|5765|  NMTR|2020-02-19|NULL|NULL|NULL| NULL|  NULL|    NULL|  Health Care|Major Pharmaceuti...|  NASDAQ|\n",
            "+----+------+----------+----+----+----+-----+------+--------+-------------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:38:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , symbol, date, open, high, low, close, volume, adjusted, sector, industry, exchange\n",
            " Schema: id, symbol, date, open, high, low, close, volume, adjusted, sector, industry, exchange\n",
            "Expected: id but found: \n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/stocks_price_final.csv\n"
          ]
        }
      ],
      "source": [
        "df5.filter(df5.open.isNull()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/02/12 19:38:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
            " Header: , symbol, date, open, high, low, close, volume, adjusted, sector, industry, exchange\n",
            " Schema: id, symbol, date, open, high, low, close, volume, adjusted, sector, industry, exchange\n",
            "Expected: id but found: \n",
            "CSV file: file:///home/iabd/Escritorio/IABD/Big-Data/BD%20Aplicado/UD%203/stocks_price_final.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+----+----+----+---+-----+------+--------+------+--------+--------+\n",
            "| id|symbol|date|open|high|low|close|volume|adjusted|sector|industry|exchange|\n",
            "+---+------+----+----+----+---+-----+------+--------+------+--------+--------+\n",
            "+---+------+----+----+----+---+-----+------+--------+------+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df5=df5.dropna(subset=[\"open\"])\n",
        "df5.filter(df5.open.isNull()).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "sapa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
