{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK - DATAFRAMES\n",
    "Un DataFrame es una estructura equivalente a una tabla de base de datos relacional, con un motor bien optimizado para el trabajo en un clúster. Los datos se almacenan en filas y columnas y ofrece un conjunto de operaciones para manipular los datos.\n",
    "\n",
    "El trabajo con DataFrames es más sencillo y eficiente que el procesamiento con RDD, por eso su uso es predominante en los nuevos desarrollos con Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREAR SparkSession Y SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 22:10:49 WARN Utils: Your hostname, AINARA-MAC.local resolves to a loopback address: 127.0.0.1; using 192.168.0.109 instead (on interface en0)\n",
      "24/01/21 22:10:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/21 22:10:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/21 22:10:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").appName(\"pyspark_dataframe\").getOrCreate()\n",
    ")\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREAR DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREAR UN DATAFRAME A PARTIR DE UN RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datos = [(\"Aitor\", 182), (\"Pedro\", 178), (\"Marina\", 161)]\n",
    "rdd = sc.parallelize(datos)\n",
    "dfRDD = rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante printSchema obtenemos un resumen del esquema del DataFrame, donde para cada columna se indica el nombre, el tipo y si admite valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRDD.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como los nombres de las columnas son _1 y _2. Para asignarle un nombre adecuado podemos pasarle una lista con los nombres a la hora de crear el DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- altura: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnas = [\"nombre\",\"altura\"]\n",
    "dfRDD = rdd.toDF(columnas)\n",
    "dfRDD.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos crear un DataFrame directamente desde una SparkSession sin crear un RDD previamente mediante el método createDataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- altura: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDesdeDatos = spark.createDataFrame(datos).toDF(*columnas)\n",
    "dfDesdeDatos.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREAR UN DATAFRAME A PARTIR DE UN FICHERO\n",
    "Lo más usual es cargar los datos desde una archivo externo. Para ello, mediante el API de DataFrameReader cargaremos los datos directamente en un Dataframe mediante diferentes métodos dependiendo del formato.\n",
    "\n",
    "Para cada formato, existe un método corto que se llama como el formato en sí, y un método general donde mediante format indicamos el formato y que finaliza con el método load siempre dentro de spark.read:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfCSV = spark.read.csv(\"Datos/pdi_sales.csv\")\n",
    "dfCSV = spark.read.csv(\"Datos/*.csv\")  # Una carpeta entera\n",
    "dfCSV = spark.read.options(sep=\";\", header=True, inferSchema=True).csv(\"Datos/pdi_sales.csv\")\n",
    "dfCSV = spark.read.format(\"csv\").load(\"Datos/pdi_sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTXT = spark.read.text(\"Datos/el_quijote.txt\")\n",
    "dfTXT = spark.read.option(\"wholetext\", True).text(\n",
    "    \"Datos/\"\n",
    ")  # cada fichero se lee entero como un registro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJSON = spark.read.json(\"Datos/datos.json\")\n",
    "dfJSON = spark.read.format(\"json\").load(\"Datos/datos.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark espera que cada documento JSON ocupe una única línea. Si cada documento ocupa más de una línea, se lo indicamos mediante la opción multiline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(multiline=True,inferSchema=True).json(\"Datos/datos.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOSTRAR LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los siguientes apartados, supongamos que queremos almacenar ciertos datos de clientes, como son su nombre y apellidos, ciudad y sueldo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes = [\n",
    "    (\"Aitor\", \"Medrano\", \"Elche\", 3000),\n",
    "    (\"Pedro\", \"Casas\", \"Elche\", 4000),\n",
    "    (\"Laura\", \"García\", \"Elche\", 5000),\n",
    "    (\"Miguel\", \"Ruiz\", \"Torrellano\", 6000),\n",
    "    (\"Isabel\", \"Guillén\", \"Alicante\", 7000),\n",
    "]\n",
    "columnas = [\"nombre\", \"apellidos\", \"ciudad\", \"sueldo\"]\n",
    "df = spark.createDataFrame(clientes, columnas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema\n",
    "Recureda mediante printSchema obtenemos un resumen del esquema del DataFrame , donde para cada columna se indica el nombre, el tipo y si admite valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- apellidos: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- sueldo: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar los datos, podemos utilizar el método show, al cual le podemos indicar o no la cantidad de registros a recuperar, así como si queremos que los datos se trunquen o no, o si los queremos mostrar en vertical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+------+\n",
      "|nombre|apellidos|ciudad|sueldo|\n",
      "+------+---------+------+------+\n",
      "| Aitor|  Medrano| Elche|  3000|\n",
      "| Pedro|    Casas| Elche|  4000|\n",
      "+------+---------+------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+------+\n",
      "|nombre|apellidos|ciudad    |sueldo|\n",
      "+------+---------+----------+------+\n",
      "|Aitor |Medrano  |Elche     |3000  |\n",
      "|Pedro |Casas    |Elche     |4000  |\n",
      "|Laura |García   |Elche     |5000  |\n",
      "|Miguel|Ruiz     |Torrellano|6000  |\n",
      "|Isabel|Guillén  |Alicante  |7000  |\n",
      "+------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------\n",
      " nombre    | Aitor   \n",
      " apellidos | Medrano \n",
      " ciudad    | Elche   \n",
      " sueldo    | 3000    \n",
      "-RECORD 1------------\n",
      " nombre    | Pedro   \n",
      " apellidos | Casas   \n",
      " ciudad    | Elche   \n",
      " sueldo    | 4000    \n",
      "-RECORD 2------------\n",
      " nombre    | Laura   \n",
      " apellidos | García  \n",
      " ciudad    | Elche   \n",
      " sueldo    | 5000    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+------+\n",
      "|nombre|apellidos|ciudad|sueldo|\n",
      "+------+---------+------+------+\n",
      "|   Ait|      Med|   Elc|   300|\n",
      "|   Ped|      Cas|   Elc|   400|\n",
      "|   Lau|      Gar|   Elc|   500|\n",
      "|   Mig|      Rui|   Tor|   600|\n",
      "|   Isa|      Gui|   Ali|   700|\n",
      "+------+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=3)  # Truncar cadenas a 3 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first y head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si sólo queremos recuperar unos pocos datos (los primeros), podemos hacer uso de head o first los cuales devuelven objetos Row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000),\n",
       " Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000),\n",
       " Row(nombre='Laura', apellidos='García', ciudad='Elche', sueldo=5000)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos obtener un valor en concreto, una vez recuperada una fila, podemos acceder a sus columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aitor\n"
     ]
    }
   ],
   "source": [
    "nom1 = df.first()[0]\n",
    "print(nom1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aitor\n"
     ]
    }
   ],
   "source": [
    "nom2 = df.first()[\"nombre\"]\n",
    "print(nom2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### describe\n",
    "Podemos obtener un sumario de los datos mediante describe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/17 15:49:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 37:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+----------+------------------+\n",
      "|summary|nombre|apellidos|    ciudad|            sueldo|\n",
      "+-------+------+---------+----------+------------------+\n",
      "|  count|     5|        5|         5|                 5|\n",
      "|   mean|  NULL|     NULL|      NULL|            5000.0|\n",
      "| stddev|  NULL|     NULL|      NULL|1581.1388300841897|\n",
      "|    min| Aitor|    Casas|  Alicante|              3000|\n",
      "|    max| Pedro|     Ruiz|Torrellano|              7000|\n",
      "+-------+------+---------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "Si únicamente nos interesa saber cuantas filas tiene nuestro DataFrame, podemos hacer uso de count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect y take\n",
    "Por último, como un DataFrame por debajo es un RDD, podemos usar collect y take conforme necesitemos y recuperar objetos de tipo Row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000),\n",
       " Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000),\n",
       " Row(nombre='Laura', apellidos='García', ciudad='Elche', sueldo=5000),\n",
       " Row(nombre='Miguel', apellidos='Ruiz', ciudad='Torrellano', sueldo=6000),\n",
       " Row(nombre='Isabel', apellidos='Guillén', ciudad='Alicante', sueldo=7000)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000),\n",
       " Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aitor\n"
     ]
    }
   ],
   "source": [
    "nom = df.collect()[0][0]\n",
    "print(nom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERSISTIR LOS DATOS\n",
    "Si lo que queremos es persistir los datos, en vez de read, utilizaremos write (de manera que obtenemos un DataFrameWriter) y si usamos la forma general usaremos el método save.\n",
    "\n",
    "Por cada partición, Spark generará un archivo de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCSV.write.mode(\"overwrite\").csv(\"Datos/dat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTXT.write.mode(\"overwrite\").text(\"Datos/dat.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJSON.write.mode(\"overwrite\").json(\"Datos/dat.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATOS Y ESQUEMAS\n",
    "El esquema completo de un DataFrame se modela mediante un StructType, el cual contiene una colección de objetos StructField. Así pues, cada columna de un DataFrame de Spark se modela mediante un objeto StructField indicando su nombre, tipo y gestión de los nulos.\n",
    "\n",
    "Hemos visto que al crear un DataFrame desde un archivo externo, podemos inferir el esquema. Si queremos crear un DataFrame desde un esquema propio utilizaremos los tipos StructType, StructField, así como el tipo necesario para cada columna. \n",
    "\n",
    "Además de cadenas (StringType), enteros (IntegerType), flotantes (FloatType) Y dobles (DoubleType), tenemos tipos booleanos (BooleanType), de fecha (DateType y TimestampType), así como tipos complejos como ArrayType, MapType y StructType."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvamos al ejemplo anterior donde tenemos ciertos datos de clientes, como son su nombre y apellidos, ciudad y sueldo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientes = [\n",
    "    (\"Aitor\", \"Medrano\", \"Elche\", 3000),\n",
    "    (\"Pedro\", \"Casas\", \"Elche\", 4000),\n",
    "    (\"Laura\", \"García\", \"Elche\", 5000),\n",
    "    (\"Miguel\", \"Ruiz\", \"Torrellano\", 6000),\n",
    "    (\"Isabel\", \"Guillén\", \"Alicante\", 7000),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta estructura, definiremos un esquema con los campos, indicando para cada uno de ellos su nombre, tipo y si admite valores nulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "esquema = StructType(\n",
    "    [\n",
    "        StructField(\"nombre\", StringType(), False),\n",
    "        StructField(\"apellidos\", StringType(), False),\n",
    "        StructField(\"ciudad\", StringType(), True),\n",
    "        StructField(\"sueldo\", IntegerType(), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ya podemos crear un DataFrame con datos propios que cumplen un esquema haciendo uso del método createDataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = false)\n",
      " |-- apellidos: string (nullable = false)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- sueldo: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfClientes = spark.createDataFrame(data=clientes, schema=esquema)\n",
    "dfClientes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo que queremos es asignarle un esquema a un DataFrame que vamos a leer desde una fuente de datos externa, hemos de emplear el método schema:\n",
    "``````\n",
    "dfClientes = spark.read.option(\"header\", True).schema(esquema).csv(\"clientes.csv\")\n",
    "``````\n",
    "\n",
    "La inferencia de los tipos de los datos es un proceso computacionalmente costoso. Por ello, si nuestro conjunto de datos es grande, es muy recomendable crear el esquema de forma programativa y configurarlo en la carga de datos.\n",
    "\n",
    "\n",
    "Respecto al esquema, tenemos diferentes propiedades como columns, dtypes y schema con las que obtener su información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nombre', 'apellidos', 'ciudad', 'sueldo']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfClientes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nombre', 'string'),\n",
       " ('apellidos', 'string'),\n",
       " ('ciudad', 'string'),\n",
       " ('sueldo', 'int')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfClientes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('nombre', StringType(), False), StructField('apellidos', StringType(), False), StructField('ciudad', StringType(), True), StructField('sueldo', IntegerType(), False)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfClientes.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si una vez hemos cargado un DataFrame queremos cambiar el tipo de una de sus columnas, podemos hacer uso del método withColumn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma larga\n",
    "dfClientes = dfClientes.withColumn(\"sueldo\", dfClientes.sueldo.cast(DoubleType()))\n",
    "# Forma corta\n",
    "dfClientes = dfClientes.withColumn(\"sueldo\", dfClientes.sueldo.cast(\"double\"))\n",
    "# ddfClientes = dfClientes.withColumn(\"fnac\", to_date(dfClientes.Date, \"M/d/yyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos un error al leer un dato que contiene un tipo no esperado, por defecto, Spark lanzará una excepción y se detendrá la lectura.\n",
    "\n",
    "Si queremos que asigne los tipos a los campos pero que no los valide, podemos pasarle el parámetro extra verifySchema a False al crear un DataFrame mediante spark.createDataFrame o enforceSchema también a False al cargar desde una fuente externa mediante spark.read, de manera que los datos que no concuerden con el tipo se quedarán nulos, vacíos o con valor 0, dependiendo del tipo de dato que tiene asignada la columna en el esquema.\n",
    "\n",
    "``````\n",
    "dfClientes = spark.read.option(\"header\", True).option(\"enforceSchema\",False).schema(esquema).csv(\"clientes.csv\")\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATAFRAME API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos un DataFrame podemos trabajar con los datos mediante un conjunto de operaciones estructuradas, muy similares al lenguaje relacional. Estas operaciones también se clasifican en transformaciones y acciones, recordando que las transformaciones utilizan una evaluación perezosa.\n",
    "\n",
    "Es muy importante tener en cuenta que todas las operaciones que vamos a realizar a continuación son immutables, es decir, nunca van a modificar el DataFrame sobre el que realizamos la transformación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los siguientes apartados, vamos a trabajar sobre el siguiente DataFrame con el fichero de ventas (pdi_sales_small.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Zip: string (nullable = true)\n",
      " |-- Units: integer (nullable = true)\n",
      " |-- Revenue: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales = (\n",
    "    spark.read.option(\"sep\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"Datos/pdi_sales_small.csv\")\n",
    ")\n",
    "dfSales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROYECTAR\n",
    "#### select\n",
    "La operación select permite indicar las columnas a recuperar pasándolas como parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|ProductID|Revenue|\n",
      "+---------+-------+\n",
      "|      725|  115.5|\n",
      "|      787|  314.9|\n",
      "|      788|  314.9|\n",
      "+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.select(\"ProductID\", \"Revenue\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos realizar cálculos (referenciando a los campos con nombreDataframe.nombreColumna) sobre las columnas y crear un alias (operación asociada a un campo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|ProductID|VentasMas10|\n",
      "+---------+-----------+\n",
      "|      725|      125.5|\n",
      "|      787|      324.9|\n",
      "|      788|      324.9|\n",
      "+---------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.select(dfSales.ProductID, (dfSales.Revenue + 10).alias(\"VentasMas10\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop\n",
    "Si tenemos un DataFrame con un gran número de columnas y queremos recuperarlas todas a excepción de unas pocas, es más cómodo utilizar la transformación drop, la cual funciona de manera opuesta a select, es decir, indicando las columnas que queremos quitar del resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+\n",
      "|ProductID|     Date|            Zip|\n",
      "+---------+---------+---------------+\n",
      "|      725|1/15/1999|41540          |\n",
      "|      787| 6/6/2002|41540          |\n",
      "|      788| 6/6/2002|41540          |\n",
      "+---------+---------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.drop(\"Units\", \"Revenue\", \"Country\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRABAJAR CON COLUMNAS\n",
    "\n",
    "Las columnas de un Dataframe son objetos de tipo Column. \n",
    "\n",
    "Podemos recuperar ciertas columnas de un DataFrame con cualquiera de las siguientes expresiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|ProductID|Revenue|\n",
      "+---------+-------+\n",
      "|      725|  115.5|\n",
      "|      787|  314.9|\n",
      "|      788|  314.9|\n",
      "|      940|  687.7|\n",
      "|      396|  857.1|\n",
      "|      734|  330.7|\n",
      "|      769|  257.2|\n",
      "|      499|  846.3|\n",
      "|     2254|   57.7|\n",
      "|       31|  761.2|\n",
      "|      475|  970.2|\n",
      "|      510|  837.1|\n",
      "|      499|  883.0|\n",
      "|      289|  866.0|\n",
      "|      702|  286.1|\n",
      "|      910|  414.7|\n",
      "|      901|  818.9|\n",
      "|      550|  404.0|\n",
      "|      559|  585.6|\n",
      "|      767|  105.0|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+\n",
      "|ProductID|Revenue|\n",
      "+---------+-------+\n",
      "|      725|  115.5|\n",
      "|      787|  314.9|\n",
      "|      788|  314.9|\n",
      "|      940|  687.7|\n",
      "|      396|  857.1|\n",
      "|      734|  330.7|\n",
      "|      769|  257.2|\n",
      "|      499|  846.3|\n",
      "|     2254|   57.7|\n",
      "|       31|  761.2|\n",
      "|      475|  970.2|\n",
      "|      510|  837.1|\n",
      "|      499|  883.0|\n",
      "|      289|  866.0|\n",
      "|      702|  286.1|\n",
      "|      910|  414.7|\n",
      "|      901|  818.9|\n",
      "|      550|  404.0|\n",
      "|      559|  585.6|\n",
      "|      767|  105.0|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+\n",
      "|ProductID|Revenue|\n",
      "+---------+-------+\n",
      "|      725|  115.5|\n",
      "|      787|  314.9|\n",
      "|      788|  314.9|\n",
      "|      940|  687.7|\n",
      "|      396|  857.1|\n",
      "|      734|  330.7|\n",
      "|      769|  257.2|\n",
      "|      499|  846.3|\n",
      "|     2254|   57.7|\n",
      "|       31|  761.2|\n",
      "|      475|  970.2|\n",
      "|      510|  837.1|\n",
      "|      499|  883.0|\n",
      "|      289|  866.0|\n",
      "|      702|  286.1|\n",
      "|      910|  414.7|\n",
      "|      901|  818.9|\n",
      "|      550|  404.0|\n",
      "|      559|  585.6|\n",
      "|      767|  105.0|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+\n",
      "|ProductID|Revenue|\n",
      "+---------+-------+\n",
      "|      725|  115.5|\n",
      "|      787|  314.9|\n",
      "|      788|  314.9|\n",
      "|      940|  687.7|\n",
      "|      396|  857.1|\n",
      "|      734|  330.7|\n",
      "|      769|  257.2|\n",
      "|      499|  846.3|\n",
      "|     2254|   57.7|\n",
      "|       31|  761.2|\n",
      "|      475|  970.2|\n",
      "|      510|  837.1|\n",
      "|      499|  883.0|\n",
      "|      289|  866.0|\n",
      "|      702|  286.1|\n",
      "|      910|  414.7|\n",
      "|      901|  818.9|\n",
      "|      550|  404.0|\n",
      "|      559|  585.6|\n",
      "|      767|  105.0|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.select(\"ProductID\", \"Revenue\").show()\n",
    "dfSales.select(dfSales.ProductID, dfSales.Revenue).show()\n",
    "dfSales.select(dfSales[\"ProductID\"], dfSales[\"Revenue\"]).show()\n",
    "dfSales.select(col(\"ProductID\"), col(\"Revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo para seleccionar columnas que creamos a partir de las conlumnas existentes, en este caso vamos a usar concat_ws para concatenar textos utilizado un separador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------------------+\n",
      "|nombreCompleto|sueldo|       nuevoSueldo|\n",
      "+--------------+------+------------------+\n",
      "| Aitor Medrano|3000.0|3300.0000000000005|\n",
      "|   Pedro Casas|4000.0|            4400.0|\n",
      "|  Laura García|5000.0|            5500.0|\n",
      "|   Miguel Ruiz|6000.0| 6600.000000000001|\n",
      "|Isabel Guillén|7000.0| 7700.000000000001|\n",
      "+--------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfClientes.select(\n",
    "    concat_ws(\" \", col(\"nombre\"), col(\"apellidos\")).alias(\"nombreCompleto\"),\n",
    "    \"sueldo\",\n",
    "    (col(\"sueldo\") * 1.1).alias(\"nuevoSueldo\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos un DataFrame, podemos añadir columnas mediante el método withColumn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country| total|\n",
      "+---------+---------+---------------+-----+-------+-------+------+\n",
      "|      725|1/15/1999|41540          |    1|  115.5|Germany| 115.5|\n",
      "|      787| 6/6/2002|41540          |    1|  314.9|Germany| 314.9|\n",
      "|      788| 6/6/2002|41540          |    1|  314.9|Germany| 314.9|\n",
      "|      940|1/15/1999|22587          |    1|  687.7|Germany| 687.7|\n",
      "|      396|1/15/1999|22587          |    1|  857.1|Germany| 857.1|\n",
      "|      734|4/10/2003|22587          |    1|  330.7|Germany| 330.7|\n",
      "|      769|2/15/1999|22587          |    1|  257.2|Germany| 257.2|\n",
      "|      499|1/15/1999|12555          |    1|  846.3|Germany| 846.3|\n",
      "|     2254|1/15/1999|40217          |    1|   57.7|Germany|  57.7|\n",
      "|       31|5/31/2002|40217          |    1|  761.2|Germany| 761.2|\n",
      "|      475|2/15/1999|13583          |    1|  970.2|Germany| 970.2|\n",
      "|      510|1/15/1999|22337          |    1|  837.1|Germany| 837.1|\n",
      "|      499| 6/5/2002|22337          |    1|  883.0|Germany| 883.0|\n",
      "|      289|2/15/1999|13587          |    1|  866.0|Germany| 866.0|\n",
      "|      702|2/15/1999|13587          |    1|  286.1|Germany| 286.1|\n",
      "|      910|3/15/1999|13587          |    1|  414.7|Germany| 414.7|\n",
      "|      901|2/15/1999|13587          |    2|  818.9|Germany|1637.8|\n",
      "|      550|1/15/1999|22177          |    1|  404.0|Germany| 404.0|\n",
      "|      559|1/15/1999|22177          |    1|  585.6|Germany| 585.6|\n",
      "|      767| 6/7/2002|22177          |    1|  105.0|Germany| 105.0|\n",
      "+---------+---------+---------------+-----+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNuevo = dfSales.withColumn(\"total\", dfSales.Units * dfSales.Revenue)\n",
    "dfNuevo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de añadir una columna con una expresión es mediante la transformación selectExpr. Por ejemplo, podemos conseguir el mismo resultado que en el ejemplo anterior de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country| total|\n",
      "+---------+---------+---------------+-----+-------+-------+------+\n",
      "|      725|1/15/1999|41540          |    1|  115.5|Germany| 115.5|\n",
      "|      787| 6/6/2002|41540          |    1|  314.9|Germany| 314.9|\n",
      "|      788| 6/6/2002|41540          |    1|  314.9|Germany| 314.9|\n",
      "|      940|1/15/1999|22587          |    1|  687.7|Germany| 687.7|\n",
      "|      396|1/15/1999|22587          |    1|  857.1|Germany| 857.1|\n",
      "|      734|4/10/2003|22587          |    1|  330.7|Germany| 330.7|\n",
      "|      769|2/15/1999|22587          |    1|  257.2|Germany| 257.2|\n",
      "|      499|1/15/1999|12555          |    1|  846.3|Germany| 846.3|\n",
      "|     2254|1/15/1999|40217          |    1|   57.7|Germany|  57.7|\n",
      "|       31|5/31/2002|40217          |    1|  761.2|Germany| 761.2|\n",
      "|      475|2/15/1999|13583          |    1|  970.2|Germany| 970.2|\n",
      "|      510|1/15/1999|22337          |    1|  837.1|Germany| 837.1|\n",
      "|      499| 6/5/2002|22337          |    1|  883.0|Germany| 883.0|\n",
      "|      289|2/15/1999|13587          |    1|  866.0|Germany| 866.0|\n",
      "|      702|2/15/1999|13587          |    1|  286.1|Germany| 286.1|\n",
      "|      910|3/15/1999|13587          |    1|  414.7|Germany| 414.7|\n",
      "|      901|2/15/1999|13587          |    2|  818.9|Germany|1637.8|\n",
      "|      550|1/15/1999|22177          |    1|  404.0|Germany| 404.0|\n",
      "|      559|1/15/1999|22177          |    1|  585.6|Germany| 585.6|\n",
      "|      767| 6/7/2002|22177          |    1|  105.0|Germany| 105.0|\n",
      "+---------+---------+---------------+-----+-------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.selectExpr(\"*\", \"Units * Revenue as total\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si por algún extraño motivo necesitamos cambiarle el nombre a una columna podemos utilizar la transformación withColumnRenamed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|     PostalCode|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      725|1/15/1999|41540          |    1|  115.5|Germany|\n",
      "|      787| 6/6/2002|41540          |    1|  314.9|Germany|\n",
      "|      788| 6/6/2002|41540          |    1|  314.9|Germany|\n",
      "|      940|1/15/1999|22587          |    1|  687.7|Germany|\n",
      "|      396|1/15/1999|22587          |    1|  857.1|Germany|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.withColumnRenamed(\"Zip\", \"PostalCode\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTRAR\n",
    "\n",
    "Si queremos eliminar filas, usaremos el método filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      725|1/15/1999|41540          |    1|  115.5|Germany|\n",
      "|      787| 6/6/2002|41540          |    1|  314.9|Germany|\n",
      "|      788| 6/6/2002|41540          |    1|  314.9|Germany|\n",
      "|      940|1/15/1999|22587          |    1|  687.7|Germany|\n",
      "|      396|1/15/1999|22587          |    1|  857.1|Germany|\n",
      "|      734|4/10/2003|22587          |    1|  330.7|Germany|\n",
      "|      769|2/15/1999|22587          |    1|  257.2|Germany|\n",
      "|      499|1/15/1999|12555          |    1|  846.3|Germany|\n",
      "|     2254|1/15/1999|40217          |    1|   57.7|Germany|\n",
      "|       31|5/31/2002|40217          |    1|  761.2|Germany|\n",
      "|      475|2/15/1999|13583          |    1|  970.2|Germany|\n",
      "|      510|1/15/1999|22337          |    1|  837.1|Germany|\n",
      "|      499| 6/5/2002|22337          |    1|  883.0|Germany|\n",
      "|      289|2/15/1999|13587          |    1|  866.0|Germany|\n",
      "|      702|2/15/1999|13587          |    1|  286.1|Germany|\n",
      "|      910|3/15/1999|13587          |    1|  414.7|Germany|\n",
      "|      901|2/15/1999|13587          |    2|  818.9|Germany|\n",
      "|      550|1/15/1999|22177          |    1|  404.0|Germany|\n",
      "|      559|1/15/1999|22177          |    1|  585.6|Germany|\n",
      "|      767| 6/7/2002|22177          |    1|  105.0|Germany|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.filter(dfSales.Country == \"Germany\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por similitud con SQL, podemos utilizar también where como un alias de filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+-----+-------+-------+\n",
      "|ProductID|      Date|            Zip|Units|Revenue|Country|\n",
      "+---------+----------+---------------+-----+-------+-------+\n",
      "|      495| 3/15/1999|75213 CEDEX 16 |   77|43194.1|France |\n",
      "|     2091| 5/15/1999|9739           |   24| 3652.7|Mexico |\n",
      "|     2091| 6/15/1999|40213          |   41| 6240.1|Germany|\n",
      "|     2091|10/15/1999|40213          |   41| 6347.7|Germany|\n",
      "|     2091|12/15/1999|40213          |   23| 3560.9|Germany|\n",
      "+---------+----------+---------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.where(dfSales.Units > 20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar los operadores lógicos (& para conjunción y | para la disyunción) para crear condiciones compuestas (recordad rodear cada condición entre paréntesis):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+-----+-------+-------+\n",
      "|ProductID|      Date|            Zip|Units|Revenue|Country|\n",
      "+---------+----------+---------------+-----+-------+-------+\n",
      "|     2091| 6/15/1999|40213          |   41| 6240.1|Germany|\n",
      "|     2091|10/15/1999|40213          |   41| 6347.7|Germany|\n",
      "|     2091|12/15/1999|40213          |   23| 3560.9|Germany|\n",
      "+---------+----------+---------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.filter((dfSales.Country == \"Germany\") & (dfSales.Units > 20)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|     2314|5/15/1999|46045          |    1|   13.9|Germany|\n",
      "|     1322| 1/6/2000|75593 CEDEX 12 |    1|  254.5|France |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.filter((dfSales.ProductID==2314) | (dfSales.ProductID==1322)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un caso particular de filtrado es la eliminación de los registros repetidos, lo cual lo podemos hacer de dos maneras:\n",
    "\n",
    "* Haciendo uso del método distinct tras haber realizado alguna transformación\n",
    "* Utilizando dropDuplicates sobre un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Country|\n",
      "+-------+\n",
      "|Germany|\n",
      "|France |\n",
      "|Canada |\n",
      "|Mexico |\n",
      "| France|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.select(\"Country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Country|\n",
      "+-------+\n",
      "|Germany|\n",
      "|France |\n",
      "|Canada |\n",
      "|Mexico |\n",
      "| France|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.dropDuplicates([\"Country\"]).select(\"Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORDENAR\n",
    "Una vez recuperados los datos deseados, podemos ordenarlos mediante sort u orderBy (son operaciones totalmente equivalentes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|ProductID|Revenue|\n",
      "+---------+-------+\n",
      "|     2314|   13.9|\n",
      "|     1974|   52.4|\n",
      "|     1974|   52.4|\n",
      "|     1974|   52.4|\n",
      "|     1974|   52.4|\n",
      "+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|     2314|5/15/1999|46045          |    1|   13.9|Germany|\n",
      "|     1974|3/15/1999|R3B            |    1|   52.4|Canada |\n",
      "|     1974|4/15/1999|R3H            |    1|   52.4|Canada |\n",
      "|     1974|3/15/1999|R3H            |    1|   52.4|Canada |\n",
      "|     1974|1/15/1999|R3S            |    1|   52.4|Canada |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      495|3/15/1999|75213 CEDEX 16 |   77|43194.1|France |\n",
      "|      495| 3/1/2000|75391 CEDEX 08 |   18|10395.0|France |\n",
      "|      464|6/11/2003|75213 CEDEX 16 |   16|10075.8|France |\n",
      "|      464| 8/1/2000|22397          |   17| 9817.5|Germany|\n",
      "|      495| 3/1/2000|06175 CEDEX 2  |   16| 9240.0|France |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      495|3/15/1999|75213 CEDEX 16 |   77|43194.1|France |\n",
      "|      495| 3/1/2000|75391 CEDEX 08 |   18|10395.0|France |\n",
      "|      464|6/11/2003|75213 CEDEX 16 |   16|10075.8|France |\n",
      "|      464| 8/1/2000|22397          |   17| 9817.5|Germany|\n",
      "|      495| 3/1/2000|06175 CEDEX 2  |   16| 9240.0|France |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      495|3/15/1999|75213 CEDEX 16 |   77|43194.1|France |\n",
      "|      495| 3/1/2000|75391 CEDEX 08 |   18|10395.0|France |\n",
      "|      464|6/11/2003|75213 CEDEX 16 |   16|10075.8|France |\n",
      "|      464| 8/1/2000|22397          |   17| 9817.5|Germany|\n",
      "|      495| 3/1/2000|06175 CEDEX 2  |   16| 9240.0|France |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      495|3/15/1999|75213 CEDEX 16 |   77|43194.1|France |\n",
      "|      495| 3/1/2000|75391 CEDEX 08 |   18|10395.0|France |\n",
      "|      464|6/11/2003|75213 CEDEX 16 |   16|10075.8|France |\n",
      "|      464| 8/1/2000|22397          |   17| 9817.5|Germany|\n",
      "|      495| 3/1/2000|06175 CEDEX 2  |   16| 9240.0|France |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.select(\"ProductID\", \"Revenue\").sort(\"Revenue\").show(5)\n",
    "dfSales.sort(\"Revenue\", ascending=True).show(5)\n",
    "\n",
    "# Ordenación descendiente\n",
    "dfSales.sort(dfSales.Revenue.desc()).show(5)\n",
    "dfSales.sort(\"Revenue\", ascending=False).show(5)\n",
    "\n",
    "# Ordenación diferente en cada columna\n",
    "dfSales.sort(dfSales.Revenue.desc(), dfSales.Units.asc()).show(5)\n",
    "dfSales.sort([\"Revenue\", \"Units\"], ascending=[0, 1]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente, tras realizar una ordenación, es habitual quedarse con un subconjunto de los datos. Para ello, podemos utilizar la transformación limit.\n",
    "\n",
    "Por ejemplo, la siguiente transformación es similar al ejemplo anterior, sólo que ahora al driver únicamente le llegan 5 registros, en vez de traerlos todos y sólo mostrar 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      495|3/15/1999|75213 CEDEX 16 |   77|43194.1|France |\n",
      "|      495| 3/1/2000|75391 CEDEX 08 |   18|10395.0|France |\n",
      "|      464|6/11/2003|75213 CEDEX 16 |   16|10075.8|France |\n",
      "|      464| 8/1/2000|22397          |   17| 9817.5|Germany|\n",
      "|      495| 3/1/2000|06175 CEDEX 2  |   16| 9240.0|France |\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales.sort(dfSales.Revenue.desc(), dfSales.Units.asc()).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPERAR COMO CONJUNTOS\n",
    "La única manera de añadir filas a un DataFrame es creando uno nuevo que sea el resultado de unir dos DataFrames que compartan el mismo esquema (mismo nombres de columnas y en el mismo orden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevasVenta = [\n",
    "    (6666, \"2022-03-24\", \"03206\", 33, 3333.33, \"Spain\"),\n",
    "    (6666, \"2022-03-25\", \"03206\", 22, 2222.22, \"Spain\"),\n",
    "]\n",
    "# Creamos un nuevo DataFrame con las nuevas Ventas\n",
    "nvDF = spark.createDataFrame(nuevasVenta)\n",
    "# Unimos los dos DataFrames\n",
    "dfUpdated = dfSales.union(nvDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando dos DataFrames como dos conjuntos, podemos emplear las operaciones union, intersect, intersectAll (mantiene los duplicados), exceptAll (mantiene los duplicados) y subtract ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COGER MUESTRAS\n",
    "Si necesitamos recoger un subconjunto de los datos, ya sea para preparar los datos para algún modelo de machine learning como para una muestra aleatoria de los mismos, podemos utilizar las siguientes transformaciones: sample y randomSplit\n",
    "\n",
    "#### sample\n",
    "Permite obtener una muestra a partir de un porcentaje (no tiene porqué obtener una cantidad exacta). También admite un semilla e indicar si queremos que pueda repetir los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120239"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSales.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12133"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muestra = dfSales.sample(0.10)\n",
    "muestra.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11919"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muestraConRepetidos = dfSales.sample(True, 0.10)\n",
    "muestraConRepetidos.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomSplit\n",
    "Recupera diferentes DataFrames cuyos tamaños en porcentaje se indican como parámetros (si no suman uno, los parámetros se normalizan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95909"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = dfSales.randomSplit([0.8, 0.2])\n",
    "dfEntrenamiento = dfs[0]\n",
    "dfPrueba = dfs[1]\n",
    "dfEntrenamiento.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24330"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPrueba.count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIMPIAR DATOS\n",
    "Hay tres formas de gestionar la ausencia de datos o los datos erróneos:\n",
    "1. Eliminar las filas que tienen valores vacíos en una o más columnas.\n",
    "2. Rellenar los valores nulos con valores que definimos nosotros.\n",
    "3. Sustituir los datos erróneos por algún valor que sepamos como gestionarlo.\n",
    "\n",
    "Vamos a ver cada uno de estos casos a partir del siguiente dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-22| NULL|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-23|03206| NULL|2222.22|  Spain|\n",
      "|     6666|2022-03-24|03206| NULL|   NULL| Espain|\n",
      "|     NULL|      NULL| NULL| NULL|   NULL|   NULL|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "malasVentas = [\n",
    "    (6666, \"2022-03-22\", \"03206\", 33, 3333.33, \"Spain\"),\n",
    "    (6666, \"2022-03-22\", None, 33, 3333.33, \"Spain\"),\n",
    "    (6666, \"2022-03-23\", \"03206\", None, 2222.22, \"Spain\"),\n",
    "    (6666, \"2022-03-24\", \"03206\", None, None, \"Espain\"),\n",
    "    (None, None, None, None, None, None),\n",
    "]\n",
    "malDF = spark.createDataFrame(\n",
    "    malasVentas, [\"ProductID\", \"Date\", \"Zip\", \"Units\", \"Revenue\", \"Country\"]\n",
    ")\n",
    "malDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber si una columna contiene nulos, podemos hacer un filtrado utilizando el método isNull sobre los campos deseados (también podemos utilizar isNotNull si queremos el caso contrario):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+-----+-------+-------+\n",
      "|ProductID|      Date| Zip|Units|Revenue|Country|\n",
      "+---------+----------+----+-----+-------+-------+\n",
      "|     6666|2022-03-22|NULL|   33|3333.33|  Spain|\n",
      "|     NULL|      NULL|NULL| NULL|   NULL|   NULL|\n",
      "+---------+----------+----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "malDF.filter(malDF.Zip.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con las filas que contengan algún dato nulo, podemos acceder a la propiedad na, y:\n",
    "\n",
    "* eliminar las filas mediante el método drop / dropna. Puede recibir \"any\" (borrará las filas que contengan algún nulo) o \"all\" (borrará las filas que todas sus columnas contengan nulos) y una lista con las columnas a considerar. También podemos indicar la cantidad de valores no nulos que ha de contener cada fila para eliminarla mediante el parámetro thresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elimina todos los nulos\n",
    "malDF.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-22| NULL|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-23|03206| NULL|2222.22|  Spain|\n",
      "|     6666|2022-03-24|03206| NULL|   NULL| Espain|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elimina las filas que todas sus columnas son nulas\n",
    "malDF.na.drop(\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-23|03206| NULL|2222.22|  Spain|\n",
      "|     6666|2022-03-24|03206| NULL|   NULL| Espain|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elimina las filas que tienen el Zip nulo\n",
    "malDF.na.drop(subset=[\"Zip\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-22| NULL|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-23|03206| NULL|2222.22|  Spain|\n",
      "|     6666|2022-03-24|03206| NULL|   NULL| Espain|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "malDF.na.drop(thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rellenar los datos nulos mediante el método fill / fillna, indicando el valor y si queremos, sobre qué columnas aplicar la modificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-22|99999|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-23|03206| NULL|2222.22|  Spain|\n",
      "|     6666|2022-03-24|03206| NULL|   NULL| Espain|\n",
      "|     NULL|      NULL|99999| NULL|   NULL|   NULL|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rellenamos los zips vacíos por 99999\n",
    "malDF.na.fill(\"99999\", subset=[\"Zip\"]).show()\n",
    "# malDF.na.fill(\"99999\", [\"Zip\"]).show()\n",
    "# malDF.fillna({\"Zip\": \"99999\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sustituir los datos erróneos mediante el método replace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+-------+-------+\n",
      "|ProductID|      Date|  Zip|Units|Revenue|Country|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "|     6666|2022-03-22|03206|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-22| NULL|   33|3333.33|  Spain|\n",
      "|     6666|2022-03-23|03206| NULL|2222.22|  Spain|\n",
      "|     6666|2022-03-24|03206| NULL|   NULL|  Spain|\n",
      "|     NULL|      NULL| NULL| NULL|   NULL|   NULL|\n",
      "+---------+----------+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cambiamos Espain por Spain\n",
    "malDF.na.replace(\"Espain\", \"Spain\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro caso muy común es realizar una operación sobre una columna para transformar su valor, por ejemplo, pasar todo el texto a minúsculas o dividir una columna entre 100 para cambiar la escala.\n",
    "\n",
    "En nuestro caso, vamos a modificar las columnas Zip y Country para realizar un trim y borrar los espacios en blanco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+-----+-------+-------+\n",
      "|ProductID|     Date|  Zip|Units|Revenue|Country|\n",
      "+---------+---------+-----+-----+-------+-------+\n",
      "|      725|1/15/1999|41540|    1|  115.5|Germany|\n",
      "|      787| 6/6/2002|41540|    1|  314.9|Germany|\n",
      "|      788| 6/6/2002|41540|    1|  314.9|Germany|\n",
      "|      940|1/15/1999|22587|    1|  687.7|Germany|\n",
      "|      396|1/15/1999|22587|    1|  857.1|Germany|\n",
      "+---------+---------+-----+-----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSales = dfSales.withColumn(\"Country\", trim(col(\"Country\"))).withColumn(\n",
    "    \"Zip\", trim(col(\"Zip\"))\n",
    ")\n",
    "dfSales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USAR SQL\n",
    "Spark soporta el SQL, ampliamente establecido en el mundo de las bases de datos.\n",
    "\n",
    "### VISTAS TEMPORALES\n",
    "\n",
    "Ya hemos visto que los DataFrames tienen una estructura similar a una tabla de una base de datos relacional. Para poder realizar consultas, necesitaremos crear vistas temporales mediante el método createTempView o createOrReplaceTempView para posteriormente realizar una consulta sobre la vista creada a través de spark.sql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+-----+-------+-------+\n",
      "|ProductID|     Date|Zip|Units|Revenue|Country|\n",
      "+---------+---------+---+-----+-------+-------+\n",
      "|      725|1/15/1999|H1B|    1|  115.4| Canada|\n",
      "|     2235|1/15/1999|H1B|    2|  131.1| Canada|\n",
      "|      713|1/15/1999|H1B|    1|  160.1| Canada|\n",
      "+---------+---------+---+-----+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. definimos la vista\n",
    "dfSales.createOrReplaceTempView(\"ventas\")\n",
    "# 2. realizamos la consulta\n",
    "ventasCanada = spark.sql(\"select * from ventas where trim(Country)='Canada'\")\n",
    "ventasCanada.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISTAS GLOBALES \n",
    "\n",
    "Las vistas temporales tienen un alcance de SparkSession, de manera que desaparecen una vez finalice la sesión que ha creado la vista. Si necesitamos tener una vista que se comparta entre todas las sesiones y que permanezca viva hasta que la aplicación Spark finalice, podemos crear una vista temporal global mediante createOrReplaceGlobalTempView\n",
    "\n",
    "Estas vistas se almacenan en la base de datos global_temp y en las consultas es necesario poner el prefijo global_temp para acceder a sus vistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+-----+-------+-------+\n",
      "|ProductID|     Date|Zip|Units|Revenue|Country|\n",
      "+---------+---------+---+-----+-------+-------+\n",
      "|      725|1/15/1999|H1B|    1|  115.4| Canada|\n",
      "|     2235|1/15/1999|H1B|    2|  131.1| Canada|\n",
      "|      713|1/15/1999|H1B|    1|  160.1| Canada|\n",
      "+---------+---------+---+-----+-------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. definimos la vista global\n",
    "dfSales.createOrReplaceGlobalTempView(\"ventasg\")\n",
    "# 2. realizamos la consulta\n",
    "ventasCanadaG = spark.sql(\n",
    "    \"select * from global_temp.ventasg where trim(Country)='Canada'\"\n",
    ")\n",
    "ventasCanadaG.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   30060|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos otra sesión y vemos como funciona\n",
    "spark.newSession().sql(\n",
    "    \"select count(*) from global_temp.ventasg where trim(Country)='Canada'\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELIMINAR VISTAS\n",
    "Para borrar una vista que hayamos creado, necesitamos acceder al Spark Catalog y utilizar el método dropTempView o dropGlobalTempView dependiendo del tipo de vista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView(\"ventas\")\n",
    "spark.catalog.dropGlobalTempView(\"ventasg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
