{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2WVOQzMza6Z"
   },
   "source": [
    "# EJERCICIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "error",
     "timestamp": 1706115836178,
     "user": {
      "displayName": "Ainara Montoya",
      "userId": "09413080192368323852"
     },
     "user_tz": -60
    },
    "id": "6T-TvrORza6g",
    "outputId": "7db8474c-38cb-4991-d42d-a8ca1adc2e89"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max, avg, countDistinct, col, row_number,collect_list,unix_timestamp, from_unixtime, month, year, split, to_timestamp, udf,sum as _sum, hour\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DoubleType,TimestampType\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 22:14:26 WARN Utils: Your hostname, AINARA-MAC.local resolves to a loopback address: 127.0.0.1; using 192.168.0.127 instead (on interface en0)\n",
      "24/01/29 22:14:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/29 22:14:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"pyspark_dataframes2\").getOrCreate()\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39W2Yok5za6p"
   },
   "source": [
    "## EJERCICIO 0\n",
    "En un documento word haz una lista de las diferentes operaciones con una breve descripción de lo que hace y un ejemplo de como se utiliza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NsZVL46za6q"
   },
   "source": [
    "## EJERCICIO 1\n",
    "Realiza las siguientes operaciones:\n",
    "* Carga el dataset de “data/stocks_price_final.csv” (usa el esquema que creaste en la anterior relación de ejercicios)\n",
    "* Calcula el mínimo y máximo de date, open, close y adjusted\n",
    "* Calcula la media de las variables open, close y adjusted por industria\n",
    "* Genera un gráfico de líneas donde se muestre la media de open por industria para los 25 primeros registros\n",
    "* Guarda en un archivo json una selección de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9YTjKkvza6r",
    "outputId": "fc5cab7b-8202-48bd-8136-c021d346cbf9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLTz_shwza6y"
   },
   "source": [
    "## EJERCICIO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC9sFbcEza6y"
   },
   "source": [
    " Sobre las movies.tsv (importa el tsv sin esquema, pero añade los nombres de la columnas al cargar el tsv):\n",
    "\n",
    "1. ¿Cuantas películas diferentes hay? Usando sql y sin usar sql.\n",
    "2. ¿En cuantas películas ha trabajado Murphy, Eddie (I)? Usando sql y sin usar sql.\n",
    "3. ¿Cuáles son los actores que han aparecido en más de 30 películas? Usando sql y sin usar sql.\n",
    "4. ¿En que película anterior a 1980 aparecen al menos 25 intérpretes? Usando sql y sin usar sql.\n",
    "5. Muestra la cantidad de películas producidas cada año (solo debe mostrar el año y la cantidad), ordenando el listado por la cantidad de forma descendente. Usando sql y sin usar sql.\n",
    "6. A partir de la consulta anterior, crea un gráfico de barras que muestre el año y la cantidad de películas, ordenados por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32qv4_jNza60"
   },
   "source": [
    "## EJERCICIO 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U4sgI5wza60"
   },
   "source": [
    "Nos han enviado un nuevo archivo llamado movie-ratings.tsv que contiene las calificaciones de las películas.\n",
    "\n",
    "1. Crea un DataFrame que contenga los datos de ambos datasets. Usando sql y sin usar sql\n",
    "2. Muestra para cada año, la película con mayor puntuación (año, título de la película, puntuación). Investiga que es Window Functions en Spark y utilizalas para devolver lo que se pide.\n",
    "3. Sobre los datos anteriores, obtén también una lista con los nombres de los intérpretes. \n",
    "4. Averigua las tres parejas de intérpretes han trabajado juntos en más ocasiones. La salida debe tener tres columnas: interprete1, interprete2 y cantidad (necesitas utilizar un self-join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7eveL5Zza61"
   },
   "source": [
    "## EJERCICIO 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP5VHgV9za62"
   },
   "source": [
    "Hemos recibido un dataset con las ventas de 2019 de una tienda americana de productos de tecnología, mediante un conjunto de ficheros en formato CSV comprimidos en salesdata.zip.\n",
    "\n",
    "1. Una vez descomprimidos los datos, crea un DataFrame con todos los datos, infiriendo el esquema.\n",
    "2. Vuelve a realizar la lectura de los datos pero con el siguiente esquema:\n",
    "\n",
    "    ``````\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    esquema = StructType([\n",
    "        StructField(\"Order ID\", IntegerType(), False),\n",
    "        StructField(\"Product\", StringType(), False),\n",
    "        StructField(\"Quantity Ordered\", IntegerType(), True),\n",
    "        StructField(\"Price Each\", DoubleType(), False),\n",
    "        StructField(\"Order Date\", StringType(), False),\n",
    "        StructField(\"Purchase Address\", StringType(), False)\n",
    "    ])\n",
    "    ``````\n",
    "3. Tras la lectura, vamos a realizar la limpieza de datos. El primer paso será renombrar la columnas para eliminar los espacios en blanco.\n",
    "4. Elimina las filas que contengan algún campo nulo.\n",
    "5. Comprueba si las cabeceras de los archivos aparecen como datos del dataset (por ejemplo, un producto cuyo nombre sea Product). Si fuera el caso, elimina dichas filas.\n",
    "6. A partir del campo dirección, crea dos nuevas columnas para almacenar la ciudad (City) y el estado (State). Por ejemplo, para la dirección 136 Church St, New York City, NY 10001, la ciudad es New York City y el estado es NY.\n",
    "7. Modifica el campo con la fecha del pedido para que su formato sea timestamp.\n",
    "8. Sobre el campo anterior, crea dos nuevas columnas, con el mes (Month) y el año (Year) del pedido.\n",
    "9. Crea una nueva columan (precio_final) muestre el precio con descuento: un 10% de descuento si la cantidad es mayor que 1 y un 5% si es uno (hazlo definiendo tu propia función de usuario). Hazlo sin sql y con sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyZPUryKza64"
   },
   "source": [
    "## EJERCICIO 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo73aG3Zza64"
   },
   "source": [
    "Una vez realizada la transformación de los datos, vamos a realizar su carga y extraer información, utilizando Spark SQL siempre que sea posible:\n",
    "\n",
    "1. Almacena los datos en formato Parquet en la carpeta salesoutput particionando los datos por año y mes. Tras ejecutar esta operación, comprueba en disco la estructura de archivos creada.\n",
    "2. Sobre los datos almacenados, realiza una nueva lectura pero solo leyendo los datos de 2019 los cuales deberían estar almacenados en ./salesdataoutput/Year=2019.\n",
    "3. Averigua cual ha sido el mes que ha recaudado más. Para ello, deberás multiplicar el precio por la cantidad de unidades, y posteriormente, realizar alguna agregación. Sobre el resultado, crea un gráfico de barras verticales para representar las ventas totales por mes\n",
    "4. Obtén un gráfico de barras horizontales con las 10 ciudades que más unidades han vendido (ordenadas, la más larga arriba)\n",
    "5. Obtén un gráfico de línea que represente la cantidad de pedidos por Horas en las que se ha realizado un pedido que contenía al menos dos productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
